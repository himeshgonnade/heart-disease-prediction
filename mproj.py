# -*- coding: utf-8 -*-
"""MProj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jxeIseQPYTi3_Q3qk37yVqjmdR6lXHxH
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

import os
print(os.listdir())

import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv("heart.csv")

type(data)

data.shape

data.head()

data.describe()

data.info()

data.sample(5)

data.isnull().sum()

data.isnull().sum().sum()

print(data.corr()["target"].abs().sort_values(ascending=False))

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score

plt.figure(figsize=(6,4))
sns.countplot(x="target", data=data)
plt.title("Heart Disease Count")
plt.show()

pd.crosstab(data.age, data.target).plot(kind="bar", figsize=(15,5))
plt.title("Heart Disease Frequency for Ages")
plt.xlabel("Age")
plt.ylabel("Frequency")
plt.show()

pd.crosstab(data.sex, data.target).plot(kind="bar", figsize=(8,5))
plt.title("Heart Disease Frequency for Sex")
plt.xlabel("Sex (0 = Female, 1 = Male)")
plt.ylabel("Frequency")
plt.legend(["No Disease", "Disease"])
plt.show()

plt.figure(figsize=(10,7))
sns.heatmap(data.corr(), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

data.hist(figsize=(15,10))
plt.show()

X = data.drop("target", axis=1)
y = data["target"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=0
)

model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

# Accuracy
print("\nAccuracy:", round(accuracy_score(y_test, y_pred)*100,2), "%")

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d")
plt.title("Confusion Matrix")
plt.show()

precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = (2 * precision * recall) / (precision + recall)

print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

plt.figure(figsize=(8,6))
plt.scatter(range(len(y_test)), y_test, label="Actual", alpha=0.7)
plt.scatter(range(len(y_pred)), y_pred, label="Predicted", alpha=0.7)
plt.title("Actual vs Predicted Output")
plt.xlabel("Test Sample Index")
plt.ylabel("Heart Disease (0 = No, 1 = Yes)")
plt.legend()
plt.show()

comparison = pd.DataFrame({"Actual": y_test.values, "Predicted": y_pred})
plt.figure(figsize=(6,4))
sns.countplot(x="Actual", hue="Predicted", data=comparison)
plt.title("Actual vs Predicted Comparison")
plt.show()

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# Scaling
scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=0
)

# Balanced Logistic Regression
model = LogisticRegression(max_iter=1000, solver='liblinear', class_weight='balanced')
model.fit(X_train, y_train)

print("Accuracy:", model.score(X_test, y_test))

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Predict probabilities
y_prob = model.predict_proba(X_test)[:, 1]

# Calculate ROC values
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
auc_score = roc_auc_score(y_test, y_prob)

# Plot ROC Curve
plt.figure()
plt.plot(fpr, tpr)
plt.plot([0, 1], [0, 1])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve (AUC = {:.2f})".format(auc_score))
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt

scaler = StandardScaler()
X = scaler.fit_transform(X)

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=0
)

# Random Forest Model
model = RandomForestClassifier(n_estimators=100, random_state=0)
model.fit(X_train, y_train)

# Prediction
y_pred = model.predict(X_test)

# Accuracy
print("Accuracy:", round(accuracy_score(y_test, y_pred)*100, 2), "%")

# Confusion Matrix
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Classification Report
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Accuracy Graph
plt.figure()
plt.bar(["Random Forest"], [accuracy_score(y_test, y_pred)])
plt.ylabel("Accuracy")
plt.title("Best Model Accuracy")
plt.show()